#!/usr/bin/env python3
"""
Combined Excel to Pinecone Pipeline
====================================

This script combines:
1. Excel parsing (from pinecone_data_structure.py)
2. Secure Pinecone upload (from secure_pinecone_upload.py)

Usage:
    python3 combined_excel_to_pinecone.py

Prerequisites:
    export PINECONE_API_KEY="your-api-key"
    export OPENAI_API_KEY="your-api-key"

    pip install pandas openpyxl pinecone-client openai tqdm python-dotenv
"""

import pandas as pd
import numpy as np
import json
import os
import sys
from typing import Dict, List, Any, Optional
from datetime import datetime
from pathlib import Path
from tqdm import tqdm

# Load environment variables from .env file
try:
    from dotenv import load_dotenv
    load_dotenv()
    print("‚úÖ Loaded .env file")
except ImportError:
    print("‚ö†Ô∏è  python-dotenv not installed, using system environment variables only")

# Check for required packages
try:
    from pinecone import Pinecone, ServerlessSpec
except ImportError:
    print("‚ùå Error: pinecone-client not installed")
    print("Install with: pip install pinecone-client")
    sys.exit(1)

try:
    from openai import OpenAI
except ImportError:
    print("‚ùå Error: openai not installed")
    print("Install with: pip install openai")
    sys.exit(1)


# =============================================================================
# PART 1: DATA STRUCTURES
# =============================================================================

class EmployeeDataStructure:
    """Comprehensive employee data structure for Pinecone"""

    def __init__(self):
        self.sabon: str = ""  # ÏÇ¨Î≤à
        self.employee_profile: Dict[str, Any] = {}
        self.commission_contracts: List[Dict[str, Any]] = []
        self.override_records: List[Dict[str, Any]] = []
        self.policy_contracts: List[Dict[str, Any]] = []
        self.performance_records: List[Dict[str, Any]] = []
        self.additional_allowances: Dict[str, Any] = {}
        self.clawback_records: List[Dict[str, Any]] = []
        self.summary_financials: Dict[str, float] = {}

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for Pinecone storage"""
        return {
            "sabon": self.sabon,
            "employee_profile": self.employee_profile,
            "commission_contracts": self.commission_contracts,
            "override_records": self.override_records,
            "policy_contracts": self.policy_contracts,
            "performance_records": self.performance_records,
            "additional_allowances": self.additional_allowances,
            "clawback_records": self.clawback_records,
            "summary_financials": self.summary_financials
        }

    def to_text_for_embedding(self) -> str:
        """Convert to comprehensive text representation for embedding"""
        parts = []

        # Employee Profile
        if self.employee_profile:
            parts.append(f"ÏÇ¨Î≤à: {self.sabon}")
            parts.append(f"ÏÇ¨ÏõêÎ™Ö: {self.employee_profile.get('ÏÇ¨ÏõêÎ™Ö', '')}")
            parts.append(f"ÏßÅÏ¢Ö: {self.employee_profile.get('ÏßÅÏ¢Ö', '')}")
            parts.append(f"ÏÜåÏÜç: {self.employee_profile.get('ÏÜåÏÜç', '')}")
            parts.append(f"ÏÜåÏÜçÍ≤ΩÎ°ú: {self.employee_profile.get('ÏÜåÏÜçÍ≤ΩÎ°ú', '')}")
            parts.append(f"ÏúÑÏ¥âÏùº: {self.employee_profile.get('ÏúÑÏ¥âÏùº', '')}")
            parts.append(f"ÏúÑÏ¥âÍµ¨Î∂Ñ: {self.employee_profile.get('ÏúÑÏ¥âÍµ¨Î∂Ñ', '')}")

        # Summary Financials
        if self.summary_financials:
            parts.append("\n## Ïû¨Î¨¥ ÏöîÏïΩ")
            parts.append(f"ÏµúÏ¢ÖÏßÄÍ∏âÏï°: {self.summary_financials.get('ÏµúÏ¢ÖÏßÄÍ∏âÏï°', 0):,.0f}Ïõê")
            parts.append(f"Ï¥ù Ïª§ÎØ∏ÏÖò: {self.summary_financials.get('Ï¥ù_Ïª§ÎØ∏ÏÖò', 0):,.0f}Ïõê")
            parts.append(f"Ï¥ù Ïò§Î≤ÑÎùºÏù¥Îìú: {self.summary_financials.get('Ï¥ù_Ïò§Î≤ÑÎùºÏù¥Îìú', 0):,.0f}Ïõê")
            parts.append(f"Ï¥ù ÏãúÏ±ÖÍ∏àÏï°: {self.summary_financials.get('Ï¥ù_ÏãúÏ±ÖÍ∏àÏï°', 0):,.0f}Ïõê")
            parts.append(f"Ï¥ù ÌôòÏàòÍ∏àÏï°: {self.summary_financials.get('Ï¥ù_ÌôòÏàòÍ∏àÏï°', 0):,.0f}Ïõê")

        # Commission Contracts Summary
        if self.commission_contracts:
            parts.append(f"\n## ÏàòÏàòÎ£å Í≥ÑÏïΩ: {len(self.commission_contracts)}Í±¥")
            total_commission = sum(c.get('ÏßÄÍ∏âÏàòÏàòÎ£å_Ìï©Í≥Ñ', 0) for c in self.commission_contracts)
            parts.append(f"Ï¥ù ÏàòÏàòÎ£å: {total_commission:,.0f}Ïõê")

            # Group by insurance company
            by_insurer = {}
            for contract in self.commission_contracts:
                insurer = contract.get('Î≥¥ÌóòÏÇ¨', '')
                if insurer not in by_insurer:
                    by_insurer[insurer] = []
                by_insurer[insurer].append(contract)

            parts.append("Î≥¥ÌóòÏÇ¨Î≥Ñ Í≥ÑÏïΩ:")
            for insurer, contracts in by_insurer.items():
                count = len(contracts)
                amount = sum(c.get('ÏßÄÍ∏âÏàòÏàòÎ£å_Ìï©Í≥Ñ', 0) for c in contracts)
                parts.append(f"  - {insurer}: {count}Í±¥, {amount:,.0f}Ïõê")

        # Override Records
        if self.override_records:
            parts.append(f"\n## Ïò§Î≤ÑÎùºÏù¥Îìú: {len(self.override_records)}Í±¥")
            total_override = sum(o.get('Ïò§Î≤ÑÎùºÏù¥Îìú_Í∏àÏï°', 0) for o in self.override_records)
            parts.append(f"Ï¥ù Ïò§Î≤ÑÎùºÏù¥Îìú: {total_override:,.0f}Ïõê")

            by_type = {}
            for override in self.override_records:
                otype = override.get('Ïò§Î≤ÑÎùºÏù¥Îìú_Ï¢ÖÎ•ò', '')
                if otype not in by_type:
                    by_type[otype] = []
                by_type[otype].append(override)

            for otype, records in by_type.items():
                count = len(records)
                amount = sum(r.get('Ïò§Î≤ÑÎùºÏù¥Îìú_Í∏àÏï°', 0) for r in records)
                parts.append(f"  - {otype}: {count}Í±¥, {amount:,.0f}Ïõê")

        # Policy Contracts
        if self.policy_contracts:
            parts.append(f"\n## ÏãúÏ±Ö Í≥ÑÏïΩ: {len(self.policy_contracts)}Í±¥")
            total_policy = sum(p.get('ÏßÄÍ∏â_Í≥Ñ', 0) for p in self.policy_contracts)
            parts.append(f"Ï¥ù ÏãúÏ±ÖÍ∏àÏï°: {total_policy:,.0f}Ïõê")

        # Additional Allowances
        if self.additional_allowances:
            parts.append("\n## Ï∂îÍ∞Ä ÏàòÎãπ")
            for key, value in self.additional_allowances.items():
                if isinstance(value, (int, float)) and value > 0:
                    parts.append(f"  - {key}: {value:,.0f}Ïõê")
                elif isinstance(value, dict) and value.get('Í∏àÏï°', 0) > 0:
                    parts.append(f"  - {key}: {value.get('Í∏àÏï°', 0):,.0f}Ïõê")

        # Clawback Records
        if self.clawback_records:
            parts.append(f"\n## ÌôòÏàò Í∏∞Î°ù: {len(self.clawback_records)}Í±¥")
            total_clawback = sum(c.get('ÌôòÏàòÍ∏àÏï°', 0) for c in self.clawback_records)
            parts.append(f"Ï¥ù ÌôòÏàòÍ∏àÏï°: {total_clawback:,.0f}Ïõê")

            by_type = {}
            for clawback in self.clawback_records:
                ctype = clawback.get('ÌôòÏàòÏú†Ìòï', '')
                if ctype not in by_type:
                    by_type[ctype] = 0
                by_type[ctype] += clawback.get('ÌôòÏàòÍ∏àÏï°', 0)

            for ctype, amount in by_type.items():
                parts.append(f"  - {ctype}: {amount:,.0f}Ïõê")

        # Performance Summary
        if self.performance_records:
            parts.append(f"\n## ÏóÖÏ†Å Í∏∞Î°ù: {len(self.performance_records)}Í±¥")

        return "\n".join(parts)


# =============================================================================
# PART 2: EXCEL PROCESSOR
# =============================================================================

class ExcelDataProcessor:
    """Process Excel sheets and create employee-centric data structures"""

    def __init__(self, excel_path: str):
        self.excel_path = excel_path
        self.xl = pd.ExcelFile(excel_path)
        self.employees: Dict[str, EmployeeDataStructure] = {}

    def safe_read_sheet(self, sheet_name: str, header: int = 0, **kwargs) -> pd.DataFrame:
        """Safely read a sheet with error handling"""
        try:
            df = pd.read_excel(self.excel_path, sheet_name=sheet_name, header=header, **kwargs)
            df = df.replace({np.nan: None})
            return df
        except Exception as e:
            print(f"  ‚ö† Error reading sheet {sheet_name}: {e}")
            return pd.DataFrame()

    def process_individual_statements(self):
        """Process Ïù∏Î≥ÑÎ™ÖÏÑ∏ (Individual Statement)"""
        print("Processing Ïù∏Î≥ÑÎ™ÖÏÑ∏ (Individual Statement)...")
        df = self.safe_read_sheet('Ïù∏Î≥ÑÎ™ÖÏÑ∏')

        for _, row in df.iterrows():
            sabon = str(row['ÏÇ¨Î≤à'])
            if sabon not in self.employees:
                self.employees[sabon] = EmployeeDataStructure()

            emp = self.employees[sabon]
            emp.sabon = sabon

            emp.employee_profile = {
                'ÏÇ¨ÏõêÎ™Ö': row.get('ÏÇ¨ÏõêÎ™Ö'),
                'ÏßÅÏ¢Ö': row.get('ÏßÅÏ¢Ö'),
                'Career_Path': row.get('Career Path'),
                'ÏÜåÏÜç': row.get('ÏÜåÏÜç'),
                'ÏÜåÏÜçÍ≤ΩÎ°ú': row.get('ÏÜåÏÜçÍ≤ΩÎ°ú'),
                'ÏúÑÏ¥âÍµ¨Î∂Ñ': row.get('ÏúÑÏ¥âÍµ¨Î∂Ñ'),
                'ÏúÑÏ¥âÏùº': str(row.get('ÏúÑÏ¥âÏùº')) if pd.notna(row.get('ÏúÑÏ¥âÏùº')) else None,
                'ÏòÅÏóÖÍ∞úÏãúÏùº': str(row.get('ÏòÅÏóÖÍ∞úÏãúÏùº')) if pd.notna(row.get('ÏòÅÏóÖÍ∞úÏãúÏùº')) else None,
                'Ìá¥ÏÇ¨ÏùºÏûê': str(row.get('Ìá¥ÏÇ¨ÏùºÏûê')) if pd.notna(row.get('Ìá¥ÏÇ¨ÏùºÏûê')) else None,
                'Î∂ÄÏßÄÍ∏âÏó¨Î∂Ä': row.get('Î∂ÄÏßÄÍ∏âÏó¨Î∂Ä'),
                'Í≥ÑÏ¢åÎ≤àÌò∏': str(row.get('Í≥ÑÏ¢åÎ≤àÌò∏')) if pd.notna(row.get('Í≥ÑÏ¢åÎ≤àÌò∏')) else None,
                'ÏùÄÌñâ': row.get('ÏùÄÌñâ'),
                'ÎßàÍ∞êÏõî': str(row.get('ÎßàÍ∞êÏõî')),
                'Ï°∞ÏßÅ': {
                    'ÌöåÏÇ¨': row.get('ÌòÑÏû¨ ÏÜåÏÜçÍ≤ΩÎ°ú_ÌöåÏÇ¨'),
                    'Íµ¨Î∂Ñ': row.get('ÌòÑÏû¨ ÏÜåÏÜçÍ≤ΩÎ°ú_Íµ¨Î∂Ñ'),
                    'Î≥∏Î∂Ä': row.get('ÌòÑÏû¨ ÏÜåÏÜçÍ≤ΩÎ°ú_Î≥∏Î∂Ä'),
                    'ÏÇ¨ÏóÖÎã®': row.get('ÌòÑÏû¨ ÏÜåÏÜçÍ≤ΩÎ°ú_ÏÇ¨ÏóÖÎã®'),
                    'Agency': row.get('ÌòÑÏû¨ ÏÜåÏÜçÍ≤ΩÎ°ú_Agency'),
                    'ÌåÄ': row.get('ÌòÑÏû¨ ÏÜåÏÜçÍ≤ΩÎ°ú_ÌåÄ')
                }
            }

            emp.summary_financials = {
                'ÏµúÏ¢ÖÏßÄÍ∏âÏï°': float(row.get('ÏµúÏ¢ÖÏßÄÍ∏âÏï°', 0) or 0),
                'Ïª§ÎØ∏ÏÖòÍ≥Ñ': float(row.get('Ïª§ÎØ∏ÏÖòÍ≥Ñ', 0) or 0),
                'FC_Ïª§ÎØ∏ÏÖòÍ≥Ñ': float(row.get('FC Ïª§ÎØ∏ÏÖòÍ≥Ñ', 0) or 0),
                'FCÍ≥ÑÏïΩÎ™®Ïßë_Ïª§ÎØ∏ÏÖò': float(row.get('FCÍ≥ÑÏïΩÎ™®Ïßë Ïª§ÎØ∏ÏÖò‚Ö°', 0) or 0),
                'ÌòÑÍ∏àÏãúÏ±Ö': float(row.get('ÌòÑÍ∏àÏãúÏ±Ö', 0) or 0),
                'FCÍ≥ÑÏïΩÏú†ÏßÄ_Ïª§ÎØ∏ÏÖò': float(row.get('FCÍ≥ÑÏïΩÏú†ÏßÄ Î∞è ÏÑúÎπÑÏä§ Ïª§ÎØ∏ÏÖò‚Ö°', 0) or 0),
                'Ïò§Î≤ÑÎùºÏù¥ÎìúÍ≥Ñ': float(row.get('Ïò§Î≤ÑÎùºÏù¥ÎìúÍ≥Ñ', 0) or 0),
                'BM_Ïò§Î≤ÑÎùºÏù¥Îìú': float(row.get('BM Ïò§Î≤ÑÎùºÏù¥Îìú‚Ö°', 0) or 0),
                'MD_Ïò§Î≤ÑÎùºÏù¥Îìú': float(row.get('MD Ïò§Î≤ÑÎùºÏù¥Îìú‚Ö°', 0) or 0),
                'ÏÇ¨ÏóÖÎã®Ïû•_Ïò§Î≤ÑÎùºÏù¥Îìú': float(row.get('ÏÇ¨ÏóÖÎã®Ïû• Ïò§Î≤ÑÎùºÏù¥Îìú‚Ö°', 0) or 0),
                'ÏßÄÏÇ¨Ïû•_Ïò§Î≤ÑÎùºÏù¥Îìú': float(row.get('ÏßÄÏÇ¨Ïû• Ïò§Î≤ÑÎùºÏù¥Îìú‚Ö°', 0) or 0),
                'Ïú†ÏπòÏûê_Ïò§Î≤ÑÎùºÏù¥Îìú': float(row.get('Ïú†ÏπòÏûê Ïò§Î≤ÑÎùºÏù¥Îìú‚Ö°', 0) or 0),
                'Í≥µÌÜµÏª§ÎØ∏ÏÖòÍ≥Ñ': float(row.get('Í≥µÌÜµÏª§ÎØ∏ÏÖòÍ≥Ñ', 0) or 0),
                'Account_Balance_ÎãπÏõîÏ†ÅÎ¶ΩÏï°': float(row.get('Account Balance ÎãπÏõîÏ†ÅÎ¶ΩÏï°', 0) or 0),
                'Account_Balance_ÏßÄÍ∏âÏï°': float(row.get('Account Balance ÏßÄÍ∏âÏï°', 0) or 0),
                'Ïù¥Ïõî_Î≥¥Ïàò_ÌôòÏàòÍ∏àÏï°': float(row.get('Ïù¥Ïõî Î≥¥Ïàò ÌôòÏàòÍ∏àÏï°', 0) or 0),
                'ÎØ∏ÌôòÏàò_Ïú†Î≥¥Í∏àÏï°': float(row.get('ÎØ∏ÌôòÏàò Ïú†Î≥¥Í∏àÏï°', 0) or 0),
                'Í∏∞ÌÉÄÏª§ÎØ∏ÏÖòÍ≥Ñ': float(row.get('Í∏∞ÌÉÄÏª§ÎØ∏ÏÖòÍ≥Ñ', 0) or 0),
                'ÏßÄÏÇ¨ÏßÄÍ∏â': float(row.get('ÏßÄÏÇ¨ÏßÄÍ∏â', 0) or 0),
                'Í≥ºÏÑ∏Í≥Ñ': float(row.get('Í≥ºÏÑ∏Í≥Ñ', 0) or 0),
                'Í≥µÏ†úÍ≥Ñ': float(row.get('Í≥µÏ†úÍ≥Ñ', 0) or 0),
                'ÏÜåÎìùÏÑ∏': float(row.get('ÏÜåÎìùÏÑ∏', 0) or 0),
                'Ï£ºÎØºÏÑ∏': float(row.get('Ï£ºÎØºÏÑ∏', 0) or 0),
                'ÏõêÏ≤úÏÑ∏': float(row.get('ÏõêÏ≤úÏÑ∏', 0) or 0),
                'Í∑ºÎ°úÏÇ∞Ïû¨Î≥¥ÌóòÎ£å': float(row.get('Í∑ºÎ°úÏÇ∞Ïû¨Î≥¥ÌóòÎ£å', 0) or 0),
                'Í≥†Ïö©Î≥¥ÌóòÎ£å': float(row.get('Í≥†Ïö©Î≥¥ÌóòÎ£å', 0) or 0)
            }

        print(f"  ‚úì Processed {len(self.employees)} employees")

    def process_commission_contracts(self):
        """Process Í±¥Î≥ÑÏàòÏàòÎ£å (Commission by Contract)"""
        print("Processing Í±¥Î≥ÑÏàòÏàòÎ£å (Commission by Contract)...")
        df = self.safe_read_sheet('Í±¥Î≥ÑÏàòÏàòÎ£å')

        contract_count = 0
        for _, row in df.iterrows():
            sabon = str(row['ÏßÄÍ∏âÏÇ¨ÏõêÎ≤àÌò∏'])
            if sabon not in self.employees:
                self.employees[sabon] = EmployeeDataStructure()
                self.employees[sabon].sabon = sabon

            emp = self.employees[sabon]

            contract = {
                'ÎßàÍ∞êÏõî': str(row.get('ÎßàÍ∞êÏõî')),
                'Î≥¥ÌóòÏÇ¨': row.get('Î≥¥ÌóòÏÇ¨'),
                'Ï¶ùÍ∂åÎ≤àÌò∏': str(row.get('Ï¶ùÍ∂åÎ≤àÌò∏')),
                'Í≥ÑÏïΩÏùº': str(row.get('Í≥ÑÏïΩÏùº')) if pd.notna(row.get('Í≥ÑÏïΩÏùº')) else None,
                'Í≥ÑÏïΩÏÉÅÌÉú': row.get('Ï≤òÎ¶¨Í≥ÑÏïΩÏÉÅÌÉú'),
                'ÎÇ©ÏûÖÌöåÏ∞®': int(row.get('Ï≤òÎ¶¨ÎÇ©ÏûÖÌöåÏ∞®', 0) or 0),
                'ÏßÄÍ∏âÎ°úÏßÅ': row.get('ÏßÄÍ∏âÎ°úÏßÅ'),
                'ÎÇ©ÏûÖÎ∞©Î≤ï': row.get('ÎÇ©ÏûÖÎ∞©Î≤ï'),
                'ÏÑ†ÏßÄÍ∏â_Î∂ÑÍ∏â': row.get('ÏÑ†ÏßÄÍ∏â/Î∂ÑÍ∏â'),
                'Í∑úÏ†ï': row.get('Í∑úÏ†ï'),
                'Î∞∞Î∂ÑÏú®': float(row.get('Î∞∞Î∂ÑÏú®', 0) or 0),
                'Î≥¥ÌóòÎ£å': float(row.get('Î≥¥ÌóòÎ£å', 0) or 0),
                'MFYC': float(row.get('MFYC', 0) or 0),
                'AFYC': float(row.get('AFYC', 0) or 0),
                'Î≥¥ÌóòÏÇ¨ÌôòÏÇ∞': float(row.get('Î≥¥ÌóòÏÇ¨ÌôòÏÇ∞', 0) or 0),
                'ÏßÄÍ∏âÏú®': float(row.get('ÏßÄÍ∏âÏú®', 0) or 0),
                'ÏßÄÍ∏âÏàòÏàòÎ£å_Î™®Ïßë': float(row.get('[ÏßÄÍ∏âÏàòÏàòÎ£å] Î™®Ïßë', 0) or 0),
                'ÏßÄÍ∏âÏàòÏàòÎ£å_Ïú†ÏßÄ': float(row.get('[ÏßÄÍ∏âÏàòÏàòÎ£å] Ïú†ÏßÄ', 0) or 0),
                'ÏßÄÍ∏âÏàòÏàòÎ£å_ÏûêÎèôÏ∞®': float(row.get('[ÏßÄÍ∏âÏàòÏàòÎ£å] ÏûêÎèôÏ∞®', 0) or 0),
                'ÏßÄÍ∏âÏàòÏàòÎ£å_ÏùºÎ∞ò': float(row.get('[ÏßÄÍ∏âÏàòÏàòÎ£å] ÏùºÎ∞ò', 0) or 0),
                'ÏßÄÍ∏âÏàòÏàòÎ£å_Ìï©Í≥Ñ': float(row.get('[ÏßÄÍ∏âÏàòÏàòÎ£å] Ìï©Í≥Ñ', 0) or 0),
                'ÏàòÏûÖÏàòÏàòÎ£å_ÏÑ±Í≥º': float(row.get('[ÏàòÏûÖÏàòÏàòÎ£å] ÏÑ±Í≥º', 0) or 0),
                'ÏàòÏûÖÏàòÏàòÎ£å_Í≥ÑÏïΩÍ¥ÄÎ¶¨': float(row.get('[ÏàòÏûÖÏàòÏàòÎ£å] Í≥ÑÏïΩÍ¥ÄÎ¶¨', 0) or 0),
                'ÏàòÏûÖÏàòÏàòÎ£å_ÏàòÍ∏à': float(row.get('[ÏàòÏûÖÏàòÏàòÎ£å] ÏàòÍ∏à', 0) or 0),
                'ÏàòÏûÖÏàòÏàòÎ£å_ÏûêÎèôÏ∞®': float(row.get('[ÏàòÏûÖÏàòÏàòÎ£å] ÏûêÎèôÏ∞®', 0) or 0),
                'ÏàòÏûÖÏàòÏàòÎ£å_ÏùºÎ∞ò': float(row.get('[ÏàòÏûÖÏàòÏàòÎ£å] ÏùºÎ∞ò', 0) or 0),
                'ÏàòÏûÖÏàòÏàòÎ£å_Ìï©Í≥Ñ': float(row.get('[ÏàòÏûÖÏàòÏàòÎ£å] Ìï©Í≥Ñ', 0) or 0),
                'ÏÉÅÌíàÍµ∞1': row.get('ÏÉÅÌíàÍµ∞1'),
                'ÏÉÅÌíàÍµ∞2': row.get('ÏÉÅÌíàÍµ∞2'),
                'ÏÉÅÌíàÎ™Ö': row.get('ÏÉÅÌíàÎ™Ö'),
                'Í≥ÑÏïΩÏûê': row.get('Í≥ÑÏïΩÏûê'),
                'ÌîºÎ≥¥ÌóòÏûê': row.get('ÌîºÎ≥¥ÌóòÏûê'),
                'ÍµêÏ∞®ÌåêÎß§': row.get('ÍµêÏ∞®ÌåêÎß§'),
                'Ïô∏Î∂ÄÏù¥Í¥Ä': row.get('Ïô∏Î∂ÄÏù¥Í¥Ä')
            }

            emp.commission_contracts.append(contract)
            contract_count += 1

        print(f"  ‚úì Processed {contract_count} commission contracts")

    def process_override_records(self):
        """Process Í±¥Î≥ÑOR (Override by Contract)"""
        print("Processing Í±¥Î≥ÑOR (Override by Contract)...")
        df = self.safe_read_sheet('Í±¥Î≥ÑOR')

        override_count = 0
        for _, row in df.iterrows():
            receiver_sabon = str(row['[Ïò§Î≤ÑÎùºÏù¥Îìú] ÎåÄÏÉÅÏûêÏÇ¨Î≤à'])
            if receiver_sabon not in self.employees:
                self.employees[receiver_sabon] = EmployeeDataStructure()
                self.employees[receiver_sabon].sabon = receiver_sabon

            emp = self.employees[receiver_sabon]

            override = {
                'ÎßàÍ∞êÏõî': str(row.get('ÎßàÍ∞êÏõî')),
                'Ïó≠Ìï†': 'receiver',
                'Ïò§Î≤ÑÎùºÏù¥Îìú_Ï¢ÖÎ•ò': row.get('[Ïò§Î≤ÑÎùºÏù¥Îìú] Ï¢ÖÎ•ò'),
                'Ïò§Î≤ÑÎùºÏù¥Îìú_ÎåÄÏÉÅÏûê': row.get('[Ïò§Î≤ÑÎùºÏù¥Îìú] ÎåÄÏÉÅÏûê'),
                'Ïò§Î≤ÑÎùºÏù¥Îìú_Í∑úÏ†ï': row.get('[Ïò§Î≤ÑÎùºÏù¥Îìú] Í∑úÏ†ï'),
                'FC_ÏÇ¨Î≤à': str(row.get('[FC] ÎåÄÏÉÅÏûêÏÇ¨Î≤à')),
                'FC_ÎåÄÏÉÅÏûê': row.get('[FC] ÎåÄÏÉÅÏûê'),
                'FC_ÏûÖÏÇ¨Ï∞®Ïõî': int(row.get('[FC] ÏûÖÏÇ¨Ï∞®Ïõî', 0) or 0),
                'FC_Í∑úÏ†ï': row.get('[FC] Í∑úÏ†ï'),
                'Î≥¥ÌóòÏÇ¨': row.get('Î≥¥ÌóòÏÇ¨'),
                'Ï¶ùÍ∂åÎ≤àÌò∏': str(row.get('Ï¶ùÍ∂åÎ≤àÌò∏')),
                'Í≥ÑÏïΩÏùº': str(row.get('Í≥ÑÏïΩÏùº')) if pd.notna(row.get('Í≥ÑÏïΩÏùº')) else None,
                'Í≥ÑÏïΩÏÉÅÌÉú': row.get('Ï≤òÎ¶¨Í≥ÑÏïΩÏÉÅÌÉú'),
                'ÎÇ©ÏûÖÌöåÏ∞®': int(row.get('Ï≤òÎ¶¨ÎÇ©ÏûÖÌöåÏ∞®', 0) or 0),
                'Í≥ÑÏÇ∞Î∞©Ïãù': row.get('Í≥ÑÏÇ∞Î∞©Ïãù'),
                'ÎÇ©ÏûÖÎ∞©Î≤ï': row.get('ÎÇ©ÏûÖÎ∞©Î≤ï'),
                'Î≥¥ÌóòÎ£å': float(row.get('Î≥¥ÌóòÎ£å', 0) or 0),
                'MFYC': float(row.get('MFYC', 0) or 0),
                'AFYC': float(row.get('AFYC', 0) or 0),
                'LPÏª§ÎØ∏ÏÖò': float(row.get('LPÏª§ÎØ∏ÏÖò', 0) or 0),
                'ÏßÄÍ∏âÏú®': float(row.get('[ÏßÄÍ∏âÏàòÏàòÎ£å] ÏßÄÍ∏âÏú®', 0) or 0),
                'Ïò§Î≤ÑÎùºÏù¥Îìú_Í∏àÏï°': float(row.get('[ÏßÄÍ∏âÏàòÏàòÎ£å] Ïò§Î≤ÑÎùºÏù¥Îìú', 0) or 0),
                'ÏàòÏûÖÏàòÏàòÎ£å_Ìï©Í≥Ñ': float(row.get('[ÏàòÏûÖÏàòÏàòÎ£å] Ìï©Í≥Ñ', 0) or 0),
                'ÏÉÅÌíàÍµ∞1': row.get('ÏÉÅÌíàÍµ∞1'),
                'ÏÉÅÌíàÍµ∞2': row.get('ÏÉÅÌíàÍµ∞2'),
                'ÏÉÅÌíàÎ™Ö': row.get('ÏÉÅÌíàÎ™Ö'),
                'Í≥ÑÏïΩÏûê': row.get('Í≥ÑÏïΩÏûê'),
                'ÌîºÎ≥¥ÌóòÏûê': row.get('ÌîºÎ≥¥ÌóòÏûê')
            }

            emp.override_records.append(override)
            override_count += 1

            # Also track in FC's record
            fc_sabon = str(row['[FC] ÎåÄÏÉÅÏûêÏÇ¨Î≤à'])
            if fc_sabon not in self.employees:
                self.employees[fc_sabon] = EmployeeDataStructure()
                self.employees[fc_sabon].sabon = fc_sabon

            fc_emp = self.employees[fc_sabon]
            fc_override = override.copy()
            fc_override['Ïó≠Ìï†'] = 'fc'
            fc_override['ÏàòÎ†πÏûê_ÏÇ¨Î≤à'] = receiver_sabon
            fc_override['ÏàòÎ†πÏûê_Ïù¥Î¶Ñ'] = row.get('[Ïò§Î≤ÑÎùºÏù¥Îìú] ÎåÄÏÉÅÏûê')
            fc_emp.override_records.append(fc_override)

        print(f"  ‚úì Processed {override_count} override records")

    def process_policy_contracts(self):
        """Process ÏãúÏ±ÖÍ±¥Î≥Ñ (Policy by Contract)"""
        print("Processing ÏãúÏ±ÖÍ±¥Î≥Ñ (Policy by Contract)...")
        df = self.safe_read_sheet('ÏãúÏ±ÖÍ±¥Î≥Ñ')

        policy_count = 0
        for _, row in df.iterrows():
            sabon = str(row['ÏÇ¨Î≤à'])
            if sabon not in self.employees:
                self.employees[sabon] = EmployeeDataStructure()
                self.employees[sabon].sabon = sabon

            emp = self.employees[sabon]

            policy = {
                'ÎßàÍ∞êÏõî': str(row.get('ÎßàÍ∞êÏõî')),
                'ÏÜåÏÜç': row.get('ÏÜåÏÜç'),
                'Î≥¥ÌóòÏÇ¨': row.get('Î≥¥ÌóòÏÇ¨'),
                'Ï¶ùÍ∂åÎ≤àÌò∏': str(row.get('Ï¶ùÍ∂åÎ≤àÌò∏')),
                'Í≥ÑÏïΩÏùºÏûê': str(row.get('Í≥ÑÏïΩÏùºÏûê')) if pd.notna(row.get('Í≥ÑÏïΩÏùºÏûê')) else None,
                'ÎÇ©ÏûÖÎ∞©Î≤ï': row.get('ÎÇ©ÏûÖ\nÎ∞©Î≤ï'),
                'Ï¥àÌöåÎ≥¥ÌóòÎ£å': float(row.get('Ï¥àÌöåÎ≥¥ÌóòÎ£å', 0) or 0),
                'CMIP': float(row.get('CMIP', 0) or 0),
                'ÏÉÅÌíàÎ™Ö': row.get('ÏÉÅÌíàÎ™Ö'),
                'Í≥ÑÏïΩÏûê': row.get('Í≥ÑÏïΩÏûê'),
                'ÌîºÎ≥¥ÌóòÏûê': row.get('ÌîºÎ≥¥ÌóòÏûê'),
                'ÎÇ©ÏûÖÍ∏∞Í∞Ñ': row.get('ÎÇ©ÏûÖÍ∏∞Í∞Ñ'),
                'ÏßÄÍ∏âÏãúÏ±Ö_Î≤ïÏù∏': float(row.get('ÏßÄÍ∏âÏãúÏ±Ö\n_Î≤ïÏù∏', 0) or 0),
                'ÏßÄÍ∏âÏãúÏ±Ö_ÏÇ¨Ïö©Ïù∏': float(row.get('ÏßÄÍ∏âÏãúÏ±Ö\n_ÏÇ¨Ïö©Ïù∏', 0) or 0),
                'ÏßÄÍ∏â_Í≥Ñ': float(row.get('ÏßÄÍ∏â Í≥Ñ', 0) or 0),
                'ÎπÑÍ≥†': row.get('ÎπÑÍ≥†')
            }

            emp.policy_contracts.append(policy)
            policy_count += 1

        print(f"  ‚úì Processed {policy_count} policy contracts")

    def process_performance_records(self):
        """Process ÏóÖÏ†Å (Performance)"""
        print("Processing ÏóÖÏ†Å (Performance)...")
        try:
            df = self.safe_read_sheet('ÏóÖÏ†Å', header=2)

            performance_count = 0
            for _, row in df.iterrows():
                sabon_fields = ['ÏàòÍ∏àLPÏÇ¨Î≤à', 'Î™®ÏßëLPÏÇ¨Î≤à', 'ÏõêÎ™®ÏßëFCÏÇ¨Î≤à']

                for field in sabon_fields:
                    if field in row and pd.notna(row[field]):
                        sabon = str(row[field])
                        if sabon not in self.employees:
                            self.employees[sabon] = EmployeeDataStructure()
                            self.employees[sabon].sabon = sabon

                        emp = self.employees[sabon]

                        perf = {
                            'Ïó≠Ìï†': field.replace('ÏÇ¨Î≤à', ''),
                            'ÏõêÎ™®ÏßëLP': row.get('ÏõêÎ™®ÏßëLP'),
                            'ÏàòÍ∏àLP': row.get('ÏàòÍ∏àLP'),
                            'ÏàòÍ∏àÏûêÏÜåÏÜç': {
                                'ÏÜåÏÜç1': row.get('ÏàòÍ∏àÏûêÏÜåÏÜç1'),
                                'ÏÜåÏÜç2': row.get('ÏàòÍ∏àÏûêÏÜåÏÜç2'),
                                'ÏÜåÏÜç3': row.get('ÏàòÍ∏àÏûêÏÜåÏÜç3'),
                                'ÏÜåÏÜç4': row.get('ÏàòÍ∏àÏûêÏÜåÏÜç4'),
                                'ÏÜåÏÜç5': row.get('ÏàòÍ∏àÏûêÏÜåÏÜç5'),
                                'ÏÜåÏÜç6': row.get('ÏàòÍ∏àÏûêÏÜåÏÜç6')
                            },
                            'Î™®ÏßëLP': row.get('Î™®ÏßëLP'),
                            'Î≥¥ÌóòÏÇ¨': row.get('Î≥¥ÌóòÏÇ¨'),
                            'ÏÉùÏÜêÎ≥¥Íµ¨Î∂Ñ': row.get('ÏÉùÏÜêÎ≥¥Íµ¨Î∂Ñ'),
                            'ÏÉÅÌíàÍµ∞1': row.get('ÏÉÅÌíàÍµ∞1'),
                            'ÏÉÅÌíàÍµ∞2': row.get('ÏÉÅÌíàÍµ∞2'),
                            'Ï¶ùÍ∂åÎ≤àÌò∏': str(row.get('Ï¶ùÍ∂åÎ≤àÌò∏')) if pd.notna(row.get('Ï¶ùÍ∂åÎ≤àÌò∏')) else None,
                            'Í≥ÑÏïΩÏùºÏûê': str(row.get('Í≥ÑÏïΩÏùºÏûê')) if pd.notna(row.get('Í≥ÑÏïΩÏùºÏûê')) else None,
                            'Í≥ÑÏïΩÏÉÅÌÉú': row.get('Í≥ÑÏïΩÏÉÅÌÉú'),
                            'Í≥ÑÏïΩÏÉÅÏÑ∏ÏÉÅÌÉú': row.get('Í≥ÑÏïΩÏÉÅÏÑ∏ÏÉÅÌÉú'),
                            'ÏÉÅÌÉúÎ≥ÄÍ≤ΩÏùº': str(row.get('ÏÉÅÌÉúÎ≥ÄÍ≤ΩÏùº')) if pd.notna(row.get('ÏÉÅÌÉúÎ≥ÄÍ≤ΩÏùº')) else None
                        }

                        emp.performance_records.append(perf)
                        performance_count += 1

            print(f"  ‚úì Processed {performance_count} performance records")
        except Exception as e:
            print(f"  ‚ö† Error processing performance records: {e}")

    def process_additional_allowances(self):
        """Process additional allowance sheets"""
        print("Processing additional allowances...")

        # ÏãúÏ±Ö2 Ïù∏Î≥ÑÎ™ÖÏÑ∏
        try:
            df = self.safe_read_sheet('ÏãúÏ±Ö2 Ïù∏Î≥ÑÎ™ÖÏÑ∏', header=4)
            for _, row in df.iterrows():
                if 'FC ÏÇ¨Î≤à' in df.columns and pd.notna(row.get('FC ÏÇ¨Î≤à')):
                    sabon = str(row['FC ÏÇ¨Î≤à'])
                    if sabon not in self.employees:
                        self.employees[sabon] = EmployeeDataStructure()
                        self.employees[sabon].sabon = sabon

                    emp = self.employees[sabon]
                    emp.additional_allowances['ÏãúÏ±Ö2_ÏßÄÏÇ¨ÏãúÏ±Ö'] = {
                        'Í∏àÏï°': float(row.get('ÏßÄÍ∏âÏï°', 0) or 0),
                        'ÎπÑÍ≥†': row.get('ÎπÑÍ≥†')
                    }
            print("  ‚úì Processed ÏãúÏ±Ö2 Ïù∏Î≥ÑÎ™ÖÏÑ∏")
        except Exception as e:
            print(f"  ‚ö† Error processing ÏãúÏ±Ö2 Ïù∏Î≥ÑÎ™ÖÏÑ∏: {e}")

        # ÏÜêÎ≥¥EXT
        try:
            df = self.safe_read_sheet('ÏÜêÎ≥¥EXT', header=4)
            for _, row in df.iterrows():
                if 'FC ÏÇ¨Î≤à' in df.columns and pd.notna(row.get('FC ÏÇ¨Î≤à')):
                    sabon = str(row['FC ÏÇ¨Î≤à'])
                    if sabon not in self.employees:
                        self.employees[sabon] = EmployeeDataStructure()
                        self.employees[sabon].sabon = sabon

                    emp = self.employees[sabon]
                    emp.additional_allowances['ÏÜêÎ≥¥EXT'] = {
                        'Í∏àÏï°': float(row.get('ÏÜêÎ≥¥ÏãúÏ±Ö Ext', 0) or 0),
                        'ÎπÑÍ≥†': row.get('ÎπÑÍ≥†')
                    }
            print("  ‚úì Processed ÏÜêÎ≥¥EXT")
        except Exception as e:
            print(f"  ‚ö† Error processing ÏÜêÎ≥¥EXT: {e}")

        # ÏàòÎãπ_Ïã†ÏûÖIP
        try:
            df = self.safe_read_sheet('ÏàòÎãπ_Ïã†ÏûÖIP(2025ÏúÑÏ¥â)', header=5)
            for _, row in df.iterrows():
                if 'FC ÏÇ¨Î≤à' in df.columns and pd.notna(row.get('FC ÏÇ¨Î≤à')):
                    sabon = str(row['FC ÏÇ¨Î≤à'])
                    if sabon not in self.employees:
                        self.employees[sabon] = EmployeeDataStructure()
                        self.employees[sabon].sabon = sabon

                    emp = self.employees[sabon]
                    emp.additional_allowances['Ïã†ÏûÖIPÏàòÎãπ'] = {
                        'Í∏àÏï°': float(row.get('ÏßÄÍ∏âÏï°', 0) or 0),
                        'Î™®ÏßëÏóÖÏ†Å': float(row.get('Î™®Ïßë(ÌôòÏÇ∞)ÏóÖÏ†Å', 0) or 0),
                        'ÏßÄÍ∏âÎåÄÏÉÅÏï°': float(row.get('ÏßÄÍ∏âÎåÄÏÉÅÏï°', 0) or 0)
                    }
            print("  ‚úì Processed ÏàòÎãπ_Ïã†ÏûÖIP")
        except Exception as e:
            print(f"  ‚ö† Error processing ÏàòÎãπ_Ïã†ÏûÖIP: {e}")

        # MGRÏÉÅÏÉù(BM)
        try:
            df = self.safe_read_sheet('MGRÏÉÅÏÉù(BM)', header=4)
            for _, row in df.iterrows():
                if 'ÏÇ¨Î≤à' in df.columns and pd.notna(row.get('ÏÇ¨Î≤à')):
                    sabon = str(row['ÏÇ¨Î≤à'])
                    if sabon not in self.employees:
                        self.employees[sabon] = EmployeeDataStructure()
                        self.employees[sabon].sabon = sabon

                    emp = self.employees[sabon]
                    emp.additional_allowances['MGRÏÉÅÏÉù'] = {
                        'Í∏àÏï°': float(row.get('ÌôúÏÑ±Ìôî ÏßÄÏõêÍ∏à', 0) or 0),
                        'Ï¥ù_Î™®ÏßëÏóÖÏ†Å': float(row.get('Ï¥ù Î™®ÏßëÏóÖÏ†Å\n(ÏÉù,ÏÜêÎ≥¥)', 0) or 0),
                        'ÏÜêÎ≥¥Î™®ÏßëÏóÖÏ†Å': float(row.get('ÏÜêÎ≥¥Î™®ÏßëÏóÖÏ†Å(‚ë†)', 0) or 0),
                        'ÏßÄÍ∏âÏú®': float(row.get('ÏßÄÍ∏âÏú®(%)', 0) or 0)
                    }
            print("  ‚úì Processed MGRÏÉÅÏÉù(BM)")
        except Exception as e:
            print(f"  ‚ö† Error processing MGRÏÉÅÏÉù(BM): {e}")

        # 13ÌöåÏ∞® Ïú†ÏßÄ(4%)
        try:
            df = self.safe_read_sheet('13ÌöåÏ∞® Ïú†ÏßÄ(4%)', header=4)
            for _, row in df.iterrows():
                if 'FC ÏÇ¨Î≤à' in df.columns and pd.notna(row.get('FC ÏÇ¨Î≤à')):
                    sabon = str(row['FC ÏÇ¨Î≤à'])
                    if sabon not in self.employees:
                        self.employees[sabon] = EmployeeDataStructure()
                        self.employees[sabon].sabon = sabon

                    retention = {
                        'Î≥¥ÌóòÏÇ¨': row.get('Î≥¥ÌóòÏÇ¨'),
                        'Ï¶ùÍ∂åÎ≤àÌò∏': str(row.get('Ï¶ùÎ≤à')) if pd.notna(row.get('Ï¶ùÎ≤à')) else None,
                        'Í≥ÑÏïΩÏùº': str(row.get('Í≥ÑÏïΩÏùº')) if pd.notna(row.get('Í≥ÑÏïΩÏùº')) else None,
                        'ÌöåÏ∞®': int(row.get('ÌöåÏ∞®', 0) or 0),
                        'ÏÉÅÌíàÎ™Ö': row.get('ÏÉÅÌíàÎ™Ö'),
                        'Ï∂îÍ∞ÄÏßÄÍ∏âÏú®': float(row.get('Ï∂îÍ∞ÄÏßÄÍ∏âÏú®', 0) or 0),
                        'ÏßÄÍ∏âÏï°': float(row.get('ÏßÄÍ∏âÏï°', 0) or 0),
                        'ÎπÑÍ≥†': row.get('ÎπÑÍ≥†')
                    }

                    emp = self.employees[sabon]
                    if '13ÌöåÏ∞®Ïú†ÏßÄ' not in emp.additional_allowances:
                        emp.additional_allowances['13ÌöåÏ∞®Ïú†ÏßÄ'] = []
                    emp.additional_allowances['13ÌöåÏ∞®Ïú†ÏßÄ'].append(retention)

            print("  ‚úì Processed 13ÌöåÏ∞® Ïú†ÏßÄ(4%)")
        except Exception as e:
            print(f"  ‚ö† Error processing 13ÌöåÏ∞® Ïú†ÏßÄ(4%): {e}")

    def process_clawback_records(self):
        """Process clawback (ÌôòÏàò) sheets"""
        print("Processing clawback records...")

        # ÌôòÏàò_ÏãúÏ±Ö2ÏßÄÍ∏âÎ∂Ñ_ÏôÑÎ£å
        try:
            df = self.safe_read_sheet('ÌôòÏàò_ÏãúÏ±Ö2ÏßÄÍ∏âÎ∂Ñ_ÏôÑÎ£å', header=5)
            for _, row in df.iterrows():
                if 'ÏÇ¨Î≤à' in df.columns and pd.notna(row.get('ÏÇ¨Î≤à')):
                    sabon = str(row['ÏÇ¨Î≤à'])
                    if sabon not in self.employees:
                        self.employees[sabon] = EmployeeDataStructure()
                        self.employees[sabon].sabon = sabon

                    emp = self.employees[sabon]

                    clawback = {
                        'ÌôòÏàòÏú†Ìòï': 'ÏãúÏ±Ö2ÏßÄÍ∏âÎ∂Ñ',
                        'Î≥¥ÌóòÏÇ¨': row.get('Î≥¥ÌóòÏÇ¨'),
                        'Ï¶ùÍ∂åÎ≤àÌò∏': str(row.get('Ï¶ùÎ≤à')) if pd.notna(row.get('Ï¶ùÎ≤à')) else None,
                        'Í≥ÑÏïΩÏùº': str(row.get('Í≥ÑÏïΩÏùº')) if pd.notna(row.get('Í≥ÑÏïΩÏùº')) else None,
                        'ÏÉÅÌíàÎ™Ö': row.get('ÏÉÅÌíàÎ™Ö'),
                        'Í≥ÑÏïΩÏÉÅÌÉú': row.get('Í≥ÑÏïΩÏÉÅÌÉú'),
                        'Ï¥àÍ∏∞_ÏõîÎÇ©P': float(row.get('Ï¥àÍ∏∞_ÏõîÎÇ©P', 0) or 0),
                        'Î≥ÄÍ≤Ω_ÏõîÎÇ©P': float(row.get('Î≥ÄÍ≤Ω_ÏõîÎÇ©P', 0) or 0),
                        'ÌôòÏàòÍ∏àÏï°': float(row.get('ÏßÄÍ∏âÏï°', 0) or 0)
                    }

                    emp.clawback_records.append(clawback)
            print("  ‚úì Processed ÌôòÏàò_ÏãúÏ±Ö2ÏßÄÍ∏âÎ∂Ñ_ÏôÑÎ£å")
        except Exception as e:
            print(f"  ‚ö† Error processing ÌôòÏàò_ÏãúÏ±Ö2ÏßÄÍ∏âÎ∂Ñ_ÏôÑÎ£å: {e}")

        # ÌôòÏàò_ÏÜåÍ∞úÎπÑ_ÏôÑÎ£å
        try:
            df = self.safe_read_sheet('ÌôòÏàò_ÏÜåÍ∞úÎπÑ_ÏôÑÎ£å', header=5)
            for _, row in df.iterrows():
                if 'ÏÇ¨Î≤à' in df.columns and pd.notna(row.get('ÏÇ¨Î≤à')):
                    sabon = str(row['ÏÇ¨Î≤à'])
                    if sabon not in self.employees:
                        self.employees[sabon] = EmployeeDataStructure()
                        self.employees[sabon].sabon = sabon

                    emp = self.employees[sabon]

                    clawback = {
                        'ÌôòÏàòÏú†Ìòï': 'ÏÜåÍ∞úÎπÑ',
                        'ÎèÑÏûÖÏûêÏÇ¨Î≤à': str(row.get('ÎèÑÏûÖÏûêÏÇ¨Î≤à')) if pd.notna(row.get('ÎèÑÏûÖÏûêÏÇ¨Î≤à')) else None,
                        'ÎèÑÏûÖÏù∏Ïõê': int(row.get('ÎèÑÏûÖÏù∏Ïõê', 0) or 0),
                        'ÏúÑÏ¥âÏùº': str(row.get('ÏúÑÏ¥âÏùº')) if pd.notna(row.get('ÏúÑÏ¥âÏùº')) else None,
                        'Ìï¥Ï¥âÏùº': str(row.get('Ìï¥Ï¥âÏùº')) if pd.notna(row.get('Ìï¥Ï¥âÏùº')) else None,
                        'ÌôòÏàòÍ∏àÏï°': float(row.get('ÌôòÏàòÎåÄÏÉÅÏï°', 0) or 0),
                    }

                    emp.clawback_records.append(clawback)
            print("  ‚úì Processed ÌôòÏàò_ÏÜåÍ∞úÎπÑ_ÏôÑÎ£å")
        except Exception as e:
            print(f"  ‚ö† Error processing ÌôòÏàò_ÏÜåÍ∞úÎπÑ_ÏôÑÎ£å: {e}")

        # ÌôòÏàò_ÍµêÏú°ÎπÑ_ÏôÑÎ£å
        try:
            df = self.safe_read_sheet('ÌôòÏàò_ÍµêÏú°ÎπÑ_ÏôÑÎ£å', header=18)
            for _, row in df.iterrows():
                if 'ÏÇ¨Î≤à' in df.columns and pd.notna(row.get('ÏÇ¨Î≤à')):
                    sabon = str(row['ÏÇ¨Î≤à'])
                    if sabon not in self.employees:
                        self.employees[sabon] = EmployeeDataStructure()
                        self.employees[sabon].sabon = sabon

                    emp = self.employees[sabon]

                    clawback = {
                        'ÌôòÏàòÏú†Ìòï': 'ÍµêÏú°ÎπÑ',
                        'ÏúÑÏ¥âÏùº': str(row.get('ÏúÑÏ¥âÏùº')) if pd.notna(row.get('ÏúÑÏ¥âÏùº')) else None,
                        'Ìï¥Ï¥âÏùº': str(row.get('Ìï¥Ï¥âÏùº')) if pd.notna(row.get('Ìï¥Ï¥âÏùº')) else None,
                        'ÌôòÏàòÍ∏àÏï°': float(row.get('ÌôòÏàòÎåÄÏÉÅÏï°', 0) or 0),
                        'Í∏∞Ï§ÄÏõî': str(row.get('Í∏∞Ï§ÄÏõî')) if pd.notna(row.get('Í∏∞Ï§ÄÏõî')) else None,
                    }

                    emp.clawback_records.append(clawback)
            print("  ‚úì Processed ÌôòÏàò_ÍµêÏú°ÎπÑ_ÏôÑÎ£å")
        except Exception as e:
            print(f"  ‚ö† Error processing ÌôòÏàò_ÍµêÏú°ÎπÑ_ÏôÑÎ£å: {e}")

    def calculate_aggregated_financials(self):
        """Calculate aggregated financial summaries for each employee"""
        print("Calculating aggregated financials...")

        for sabon, emp in self.employees.items():
            Ï¥ù_Ïª§ÎØ∏ÏÖò = sum(c.get('ÏßÄÍ∏âÏàòÏàòÎ£å_Ìï©Í≥Ñ', 0) for c in emp.commission_contracts)
            Ï¥ù_Ïò§Î≤ÑÎùºÏù¥Îìú = sum(
                o.get('Ïò§Î≤ÑÎùºÏù¥Îìú_Í∏àÏï°', 0)
                for o in emp.override_records
                if o.get('Ïó≠Ìï†') == 'receiver'
            )
            Ï¥ù_ÏãúÏ±ÖÍ∏àÏï° = sum(p.get('ÏßÄÍ∏â_Í≥Ñ', 0) for p in emp.policy_contracts)
            Ï¥ù_ÌôòÏàòÍ∏àÏï° = sum(c.get('ÌôòÏàòÍ∏àÏï°', 0) for c in emp.clawback_records)

            emp.summary_financials['Ï¥ù_Ïª§ÎØ∏ÏÖò'] = Ï¥ù_Ïª§ÎØ∏ÏÖò
            emp.summary_financials['Ï¥ù_Ïò§Î≤ÑÎùºÏù¥Îìú'] = Ï¥ù_Ïò§Î≤ÑÎùºÏù¥Îìú
            emp.summary_financials['Ï¥ù_ÏãúÏ±ÖÍ∏àÏï°'] = Ï¥ù_ÏãúÏ±ÖÍ∏àÏï°
            emp.summary_financials['Ï¥ù_ÌôòÏàòÍ∏àÏï°'] = Ï¥ù_ÌôòÏàòÍ∏àÏï°
            emp.summary_financials['Í≥ÑÏïΩÍ±¥Ïàò'] = len(emp.commission_contracts)
            emp.summary_financials['Ïò§Î≤ÑÎùºÏù¥ÎìúÍ±¥Ïàò'] = len([o for o in emp.override_records if o.get('Ïó≠Ìï†') == 'receiver'])
            emp.summary_financials['ÏãúÏ±ÖÍ≥ÑÏïΩÍ±¥Ïàò'] = len(emp.policy_contracts)
            emp.summary_financials['ÌôòÏàòÍ±¥Ïàò'] = len(emp.clawback_records)

        print(f"  ‚úì Calculated financials for {len(self.employees)} employees")

    def process_all(self):
        """Process all sheets in order"""
        print("\n" + "="*80)
        print("STEP 1: PROCESSING EXCEL DATA")
        print("="*80 + "\n")

        self.process_individual_statements()
        self.process_commission_contracts()
        self.process_override_records()
        self.process_policy_contracts()
        self.process_performance_records()
        self.process_additional_allowances()
        self.process_clawback_records()
        self.calculate_aggregated_financials()

        print("\n" + "="*80)
        print(f"EXCEL PROCESSING COMPLETE: {len(self.employees)} employees")
        print("="*80 + "\n")

        return self.employees

    def generate_employee_centric_documents(self) -> List[Dict[str, Any]]:
        """Generate employee-centric documents for Pinecone upload"""
        documents = []

        for sabon, emp in self.employees.items():
            # Create main profile document
            doc = {
                'id': f"{sabon}_profile",
                'doc_type': 'employee_profile',
                'text': emp.to_text_for_embedding(),
                'metadata': {
                    'ÏÇ¨Î≤à': sabon,
                    'ÏÇ¨ÏõêÎ™Ö': emp.employee_profile.get('ÏÇ¨ÏõêÎ™Ö', ''),
                    'ÏßÅÏ¢Ö': emp.employee_profile.get('ÏßÅÏ¢Ö', ''),
                    'ÏÜåÏÜç': emp.employee_profile.get('ÏÜåÏÜç', ''),
                    'ÏúÑÏ¥âÏùº': emp.employee_profile.get('ÏúÑÏ¥âÏùº', ''),
                    'ÏµúÏ¢ÖÏßÄÍ∏âÏï°': emp.summary_financials.get('ÏµúÏ¢ÖÏßÄÍ∏âÏï°', 0),
                    'Ï¥ù_Ïª§ÎØ∏ÏÖò': emp.summary_financials.get('Ï¥ù_Ïª§ÎØ∏ÏÖò', 0),
                    'Ï¥ù_Ïò§Î≤ÑÎùºÏù¥Îìú': emp.summary_financials.get('Ï¥ù_Ïò§Î≤ÑÎùºÏù¥Îìú', 0),
                    'Í≥ÑÏïΩÍ±¥Ïàò': emp.summary_financials.get('Í≥ÑÏïΩÍ±¥Ïàò', 0),
                }
            }
            documents.append(doc)

        return documents

    def print_summary(self):
        """Print summary statistics"""
        print("\n" + "="*80)
        print("DATA SUMMARY")
        print("="*80)

        print(f"\nTotal Employees: {len(self.employees)}")

        total_commission = sum(len(emp.commission_contracts) for emp in self.employees.values())
        total_override = sum(len(emp.override_records) for emp in self.employees.values())
        total_policy = sum(len(emp.policy_contracts) for emp in self.employees.values())
        total_performance = sum(len(emp.performance_records) for emp in self.employees.values())
        total_clawback = sum(len(emp.clawback_records) for emp in self.employees.values())

        print(f"\nTotal Records:")
        print(f"  - Commission Contracts: {total_commission:,}")
        print(f"  - Override Records: {total_override:,}")
        print(f"  - Policy Contracts: {total_policy:,}")
        print(f"  - Performance Records: {total_performance:,}")
        print(f"  - Clawback Records: {total_clawback:,}")

        total_final_payment = sum(
            emp.summary_financials.get('ÏµúÏ¢ÖÏßÄÍ∏âÏï°', 0)
            for emp in self.employees.values()
        )
        total_commission_sum = sum(
            emp.summary_financials.get('Ï¥ù_Ïª§ÎØ∏ÏÖò', 0)
            for emp in self.employees.values()
        )
        total_override_sum = sum(
            emp.summary_financials.get('Ï¥ù_Ïò§Î≤ÑÎùºÏù¥Îìú', 0)
            for emp in self.employees.values()
        )

        print(f"\nFinancial Summary:")
        print(f"  - Total Final Payments: {total_final_payment:,.0f} Ïõê")
        print(f"  - Total Commissions: {total_commission_sum:,.0f} Ïõê")
        print(f"  - Total Overrides: {total_override_sum:,.0f} Ïõê")


# =============================================================================
# PART 3: PINECONE UPLOADER
# =============================================================================

class SecurePineconeUploader:
    """Secure uploader with namespace isolation"""

    def __init__(
        self,
        index_name: str = "employee-compensation",
        embedding_model: str = "text-embedding-3-large",
        dimension: int = 3072
    ):
        self.validate_environment()

        self.pc = Pinecone(api_key=os.environ["PINECONE_API_KEY"])
        self.openai_client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])

        self.index_name = index_name
        self.embedding_model = embedding_model
        self.dimension = dimension

        self.setup_index()
        self.index = self.pc.Index(self.index_name)

    def validate_environment(self):
        """Validate required environment variables"""
        required = ["PINECONE_API_KEY", "OPENAI_API_KEY"]
        missing = [var for var in required if not os.environ.get(var)]

        if missing:
            print("‚ùå Missing required environment variables:")
            for var in missing:
                print(f"   - {var}")
            print("\nSet them using:")
            print("   export PINECONE_API_KEY='your-key'")
            print("   export OPENAI_API_KEY='your-key'")
            sys.exit(1)

    def setup_index(self):
        """Verify index exists and get its specs"""
        try:
            indexes = list(self.pc.list_indexes())
            existing_index = next((idx for idx in indexes if idx.name == self.index_name), None)

            if existing_index:
                print(f"‚úÖ Using existing index: {self.index_name}")
                print(f"   Dimension: {existing_index.dimension}")
                print(f"   Metric: {existing_index.metric}")

                if existing_index.dimension != self.dimension:
                    print(f"\n‚ö†Ô∏è  Warning: Index dimension ({existing_index.dimension}) differs from embedding model ({self.dimension})")
                    print(f"   Updating to use index dimension: {existing_index.dimension}")
                    self.dimension = existing_index.dimension

                    if self.dimension == 1536:
                        print(f"   Switching to text-embedding-3-small (1536 dimensions)")
                        self.embedding_model = "text-embedding-3-small"
            else:
                print(f"‚ùå Index '{self.index_name}' not found")
                print(f"\nAvailable indexes:")
                for idx in indexes:
                    print(f"   - {idx.name} (dimension: {idx.dimension})")
                raise ValueError(f"Index '{self.index_name}' does not exist")

        except Exception as e:
            print(f"‚ùå Error checking index: {e}")
            raise

    def generate_embeddings_batch(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings in batch"""
        if len(texts) > 2048:
            raise ValueError("OpenAI supports max 2048 texts per batch")

        response = self.openai_client.embeddings.create(
            model=self.embedding_model,
            input=texts,
            encoding_format="float"
        )

        return [item.embedding for item in response.data]

    def upload_documents(
        self,
        documents: List[Dict[Any, Any]],
        batch_size: int = 100
    ):
        """Securely upload documents with namespace isolation"""
        print("\n" + "="*80)
        print("STEP 2: SECURE PINECONE UPLOAD")
        print("="*80)

        # Group documents by employee
        docs_by_employee = {}
        for doc in documents:
            sabon = doc['metadata']['ÏÇ¨Î≤à']
            if sabon not in docs_by_employee:
                docs_by_employee[sabon] = []
            docs_by_employee[sabon].append(doc)

        print(f"\nüìä Upload Statistics:")
        print(f"   Total employees: {len(docs_by_employee)}")
        print(f"   Total documents: {len(documents)}")
        print(f"   Avg docs per employee: {len(documents) / len(docs_by_employee):.1f}")
        print(f"   Batch size: {batch_size}")
        print(f"   Embedding model: {self.embedding_model}")
        print(f"   Embedding dimension: {self.dimension}")
        print(f"\nüîê Security: Namespace isolation per employee")

        total_uploaded = 0
        total_cost = 0.0

        for idx, (sabon, employee_docs) in enumerate(tqdm(docs_by_employee.items(), desc="Uploading employees"), 1):
            namespace = f"employee_{sabon}"
            ÏÇ¨ÏõêÎ™Ö = employee_docs[0]['metadata'].get('ÏÇ¨ÏõêÎ™Ö', 'Unknown')

            # Process in batches
            for batch_idx in range(0, len(employee_docs), batch_size):
                batch = employee_docs[batch_idx:batch_idx + batch_size]

                texts = [doc['text'] for doc in batch]

                total_chars = sum(len(text) for text in texts)
                estimated_tokens = total_chars / 4
                estimated_cost = (estimated_tokens / 1000) * 0.00013

                try:
                    embeddings = self.generate_embeddings_batch(texts)
                    total_cost += estimated_cost
                except Exception as e:
                    print(f"\n‚ùå Error generating embeddings for {ÏÇ¨ÏõêÎ™Ö}: {e}")
                    continue

                vectors = []
                for doc, embedding in zip(batch, embeddings):
                    doc_sabon = doc['metadata']['ÏÇ¨Î≤à']
                    if doc_sabon != sabon:
                        raise ValueError(
                            f"üö® Security Error: Document ÏÇ¨Î≤à ({doc_sabon}) "
                            f"doesn't match employee ({sabon})"
                        )

                    vector = {
                        'id': doc['id'],
                        'values': embedding,
                        'metadata': {
                            'ÏÇ¨Î≤à': doc['metadata']['ÏÇ¨Î≤à'],
                            'ÏÇ¨ÏõêÎ™Ö': doc['metadata']['ÏÇ¨ÏõêÎ™Ö'],
                            'doc_type': doc['doc_type'],
                            **{k: v for k, v in doc['metadata'].items()
                               if k not in ['ÏÇ¨Î≤à', 'ÏÇ¨ÏõêÎ™Ö', 'doc_type'] and v is not None}
                        }
                    }
                    vectors.append(vector)

                try:
                    self.index.upsert(
                        vectors=vectors,
                        namespace=namespace
                    )
                    total_uploaded += len(vectors)
                except Exception as e:
                    print(f"\n‚ùå Error uploading for {ÏÇ¨ÏõêÎ™Ö}: {e}")

        print("\n" + "="*80)
        print("üìä UPLOAD COMPLETE")
        print("="*80)
        print(f"\n‚úÖ Successfully uploaded: {total_uploaded} vectors")
        print(f"üí∞ Estimated embedding cost: ${total_cost:.4f}")

        self.verify_upload()

    def verify_upload(self):
        """Verify upload with index statistics"""
        print("\nüîç VERIFICATION")

        try:
            stats = self.index.describe_index_stats()

            print(f"\nüìä Index Statistics:")
            print(f"   Total vectors: {stats['total_vector_count']:,}")
            print(f"   Dimension: {stats['dimension']}")
            print(f"   Namespaces: {len(stats['namespaces'])}")

            print(f"\n‚úÖ Verification complete")

        except Exception as e:
            print(f"\n‚ö†Ô∏è  Could not verify upload: {e}")


# =============================================================================
# MAIN EXECUTION
# =============================================================================

def main():
    """Main execution - combines Excel parsing and Pinecone upload"""
    import argparse
    parser = argparse.ArgumentParser(description='Combined Excel to Pinecone Pipeline')
    parser.add_argument('-y', '--yes', action='store_true', help='Skip confirmation prompt')
    args = parser.parse_args()

    print("="*80)
    print("COMBINED EXCEL TO PINECONE PIPELINE")
    print("="*80)

    # Configuration
    EXCEL_PATH = '/Users/kjyoo/JISA_V3/‚òÖ202509ÎßàÍ∞ê_HO&F Í±¥Î≥Ñ Î∞è Î™ÖÏÑ∏_20251023_Î∞∞Ìè¨Ïö©_ÏàòÎèÑÍ∂å, AL.xlsx'
    INDEX_NAME = "hof-branch-chatbot"
    EMBEDDING_MODEL = "text-embedding-3-large"
    DIMENSION = 3072
    BATCH_SIZE = 100
    SKIP_CONFIRM = args.yes

    # Check if Excel file exists
    if not Path(EXCEL_PATH).exists():
        print(f"‚ùå Excel file not found: {EXCEL_PATH}")
        sys.exit(1)

    print(f"\nüìÅ Excel file: {EXCEL_PATH}")
    print(f"üéØ Target index: {INDEX_NAME}")
    print(f"üî¢ Embedding model: {EMBEDDING_MODEL}")

    # Step 1: Process Excel
    print("\n" + "="*80)
    processor = ExcelDataProcessor(EXCEL_PATH)
    employees = processor.process_all()
    processor.print_summary()

    # Generate documents for upload
    print("\nüìÑ Generating employee-centric documents...")
    documents = processor.generate_employee_centric_documents()
    print(f"   Generated {len(documents)} documents")

    # Step 2: Upload to Pinecone
    print("\n" + "="*80)
    print(f"\n‚ö†Ô∏è  You are about to upload {len(documents)} documents to Pinecone")
    print(f"   Index: {INDEX_NAME}")
    print(f"   Embedding model: {EMBEDDING_MODEL}")
    print(f"   Estimated cost: ${(len(documents) * 500 / 4 / 1000 * 0.00013):.4f}")

    if not SKIP_CONFIRM:
        response = input("\n   Proceed with upload? (yes/no): ")
        if response.lower() not in ['yes', 'y']:
            print("\n‚ùå Upload cancelled")
            sys.exit(0)
    else:
        print("\n   Auto-confirmed with --yes flag")

    # Initialize uploader and upload
    uploader = SecurePineconeUploader(
        index_name=INDEX_NAME,
        embedding_model=EMBEDDING_MODEL,
        dimension=DIMENSION
    )

    uploader.upload_documents(
        documents=documents,
        batch_size=BATCH_SIZE
    )

    print("\n" + "="*80)
    print("‚úÖ ALL COMPLETE")
    print("="*80)
    print(f"\nüéâ Pipeline completed successfully!")
    print(f"\nüìä Summary:")
    print(f"   - Employees processed: {len(employees)}")
    print(f"   - Documents uploaded: {len(documents)}")
    print(f"   - Target index: {INDEX_NAME}")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n‚ùå Cancelled by user")
        sys.exit(1)
    except Exception as e:
        print(f"\n\n‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
